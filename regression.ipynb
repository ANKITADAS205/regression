{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Simple Linear Regression models the relationship between two variables using a straight line.\n",
        "\n",
        "2. Key assumptions include linearity, independence, homoscedasticity (constant variance), and normality of residuals.\n",
        "\n",
        "3. The coefficient *m* represents the slope — how much *Y* changes with a one-unit increase in *X*.\n",
        "\n",
        "4. The intercept *c* is the value of *Y* when *X* is zero — where the line crosses the Y-axis.\n",
        "\n",
        "5. The slope *m* is calculated using the formula:\n",
        "   $m = \\frac{n(\\sum XY) - (\\sum X)(\\sum Y)}{n(\\sum X^2) - (\\sum X)^2}$\n",
        "\n",
        "6. The least squares method minimizes the sum of squared differences between actual and predicted values.\n",
        "\n",
        "7. R² tells us how well the model explains the variance in the target variable. Closer to 1 means better fit.\n",
        "\n",
        "8. Multiple Linear Regression involves more than one independent variable to predict a dependent variable.\n",
        "\n",
        "9. Simple Linear Regression uses one predictor; Multiple Linear Regression uses two or more.\n",
        "\n",
        "10. Key assumptions include linearity, no multicollinearity, homoscedasticity, and normality of residuals.\n",
        "\n",
        "11. Heteroscedasticity means unequal variance of errors. It can distort standard errors and affect model validity.\n",
        "\n",
        "12. High multicollinearity can be reduced by removing correlated features, using PCA, or regularization.\n",
        "\n",
        "13. Use label encoding or one-hot encoding to convert categorical variables for regression models.\n",
        "\n",
        "14. Interaction terms allow us to capture the combined effect of variables that influence the outcome.\n",
        "\n",
        "15. In Simple Regression, the intercept is the value of Y when X=0. In Multiple Regression, it's when all X=0.\n",
        "\n",
        "16. The slope tells us the impact of a predictor variable on the target. Higher slope means stronger effect.\n",
        "\n",
        "17. The intercept shows the expected value of the dependent variable when all predictors are zero.\n",
        "\n",
        "18. R² doesn’t account for overfitting or number of variables. Adjusted R² is better for model comparison.\n",
        "\n",
        "19. A large standard error means the estimate of the coefficient is uncertain and may not be reliable.\n",
        "\n",
        "20. In residual plots, heteroscedasticity appears as a funnel shape. It’s important to fix for valid inferences.\n",
        "\n",
        "21. High R² but low adjusted R² suggests irrelevant variables may be inflating model performance artificially.\n",
        "\n",
        "22. Scaling helps bring variables to the same scale, improving model stability and interpretability.\n",
        "\n",
        "23. Polynomial regression models nonlinear relationships using powers of the independent variable(s).\n",
        "\n",
        "24. Unlike linear regression, polynomial regression fits curves, not straight lines.\n",
        "\n",
        "25. It’s used when data shows curvature and cannot be well-fitted with a straight line.\n",
        "\n",
        "26. General equation:\n",
        "    $Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n$\n",
        "\n",
        "27. Yes, polynomial regression can be extended to multiple variables using interaction and power terms.\n",
        "\n",
        "28. It can overfit with high degrees and becomes sensitive to outliers and noise.\n",
        "\n",
        "29. Use cross-validation, adjusted R², AIC/BIC, or validation curves to choose the right polynomial degree.\n",
        "\n",
        "30. Visualization helps detect underfitting/overfitting and ensures the model fits the trend.\n",
        "\n",
        "31. In Python, use `PolynomialFeatures` from `sklearn.preprocessing` and fit with `LinearRegression`.\n"
      ],
      "metadata": {
        "id": "0O-6Jh8LZk_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nia_yYYmZeTN"
      },
      "outputs": [],
      "source": []
    }
  ]
}